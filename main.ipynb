{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import just a function \n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import a whole library as a new name\n",
    "import pandas as pd\n",
    "\n",
    "# import package as its own name\n",
    "import nltk\n",
    "\n",
    "# other packages\n",
    "import re\n",
    " \n",
    "# impurity function\n",
    "RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')\n",
    "\n",
    "def impurity(text, min_len=10):\n",
    "    \"\"\"returns the share of suspicious characters in a text\"\"\"\n",
    "    if text == None or len(text) < min_len:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(RE_SUSPICIOUS.findall(text))/len(text)\n",
    "\n",
    "# rest of stuff\n",
    "import textacy.preprocessing as tprep\n",
    "\n",
    "def normalize(text):\n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    text = tprep.replace.phone_numbers(text)\n",
    "    text = tprep.replace.urls(text)\n",
    "    text = tprep.replace.emails(text)\n",
    "    text = tprep.replace.user_handles(text)\n",
    "    text = tprep.replace.emojis(text)\n",
    "    return text\n",
    "    \n",
    "# install pyspellchecker !!!\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import textacy\n",
    "\n",
    "from itertools import chain\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import just a function \n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import a whole library as a new name\n",
    "import pandas as pd\n",
    "\n",
    "# import package as its own name\n",
    "import nltk\n",
    "\n",
    "# other packages\n",
    "import re\n",
    " \n",
    "# impurity function\n",
    "RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')\n",
    "\n",
    "def impurity(text, min_len=10):\n",
    "    \"\"\"returns the share of suspicious characters in a text\"\"\"\n",
    "    if text == None or len(text) < min_len:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(RE_SUSPICIOUS.findall(text))/len(text)\n",
    "\n",
    "# rest of stuff\n",
    "import textacy.preprocessing as tprep\n",
    "\n",
    "def normalize(text):\n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    text = tprep.replace.phone_numbers(text)\n",
    "    text = tprep.replace.urls(text)\n",
    "    text = tprep.replace.emails(text)\n",
    "    text = tprep.replace.user_handles(text)\n",
    "    text = tprep.replace.emojis(text)\n",
    "    return text\n",
    "    \n",
    "# install pyspellchecker !!!\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import textacy\n",
    "\n",
    "from itertools import chain\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Patrick Mahomes' fiery message after win over Bills: 'They got what they asked for' | Fox News\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# myurl = \"https://www.foxnews.com/sports/patrick-mahomes-fiery-message-win-bills-they-got-what-they-asked-for\"\n",
    "# #myurl = \"https://www.foxnews.com/lifestyle/newly-elected-school-board-pennsylvania-reclaims-indigenous-mascot-rejects-cancel-culture\"\n",
    "\n",
    "# html = urlopen(myurl).read()\n",
    "\n",
    "# soupified = BeautifulSoup(html, \"html.parser\")\n",
    "# # soupified\n",
    "\n",
    "# # just try get_text()\n",
    "# try_text = soupified.get_text()\n",
    "# try_text[0:100]\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import socket\n",
    "\n",
    "# Set the timeout duration in seconds\n",
    "timeout_value = 100  # Specify the desired timeout value\n",
    "\n",
    "# Set the URL to fetch\n",
    "myurl = \"https://www.foxnews.com/sports/patrick-mahomes-fiery-message-win-bills-they-got-what-they-asked-for\"\n",
    "# myurl = \"https://themightymarketer.com/10-ways-to-get-500-connections-on-linkedin-fast/\"\n",
    "\n",
    "# Open the URL with a timeout\n",
    "try:\n",
    "    html = urlopen(myurl, timeout=timeout_value).read()\n",
    "    soupified = BeautifulSoup(html, \"html.parser\")\n",
    "    try_text = soupified.get_text()\n",
    "    print(try_text[0:100])  # Print the first 100 characters of the text\n",
    "except socket.timeout:\n",
    "    print(\"Timeout error: The request took too long to complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find an exact match for the first time this occurs\n",
    "text = try_text[\n",
    "  # everything from the end of this sentence and on\n",
    "  re.search(\"To access the content, check your email and follow the instructions provided.\", try_text).end():\n",
    "  # re.search(\"LinkedIn has chosen 500 as the “magic” number of connections.\", try_text).end():\n",
    "  # now the end\n",
    "  # re.search(\"And having connections in common with the person who is doing the search is the #2 criterion LinkedIn uses to rank search results.\", try_text).start()\n",
    "  re.search(\"CLICK HERE TO GET THE FOX NEWS APP\", try_text).start()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user7\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n       Having trouble?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Click here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kansas City Chiefs quarterback Patrick Mahomes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mahomes, surrounded by Travis Kelce, Chris Jon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They have the Baltimore Ravens up next.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence\n",
       "0                           \\n       Having trouble?\n",
       "1                                        Click here.\n",
       "2  Kansas City Chiefs quarterback Patrick Mahomes...\n",
       "3  Mahomes, surrounded by Travis Kelce, Chris Jon...\n",
       "4            They have the Baltimore Ravens up next."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# break down into sentences and put into DF\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "type(sentences)\n",
    "\n",
    "# convert to dataframe\n",
    "DF = pd.DataFrame(sentences, columns = [\"sentence\"])\n",
    "DF.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "554"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do this on the full text not broken into sentences\n",
    "len(nltk.word_tokenize(text))\n",
    "# be sure to import nltk in the proposal \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n       Having trouble?</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Click here.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kansas City Chiefs quarterback Patrick Mahomes...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mahomes, surrounded by Travis Kelce, Chris Jon...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They have the Baltimore Ravens up next.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Hey, they asked for it, and they got what the...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"But he (Andy Reid) said it.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This s--- ain’t done.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We come back next week ready to f---ing go.\"</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CLICK HERE FOR MORE SPORTS COVERAGE ON FOXNEWS...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>VIEW THE MOMENT ON X.Reid said it’s not time f...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>They still have to get through the Ravens and ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Ravens are going to be the Chiefs’ toughes...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Kansas City will have to do it on the road.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Ravens are hosting a conference championsh...</td>\n",
       "      <td>0.002222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Baltimore boasts one of the toughest defenses ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"There’s no weakness there,\" the star quarterb...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\"It’s going to take our best effort.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Defense, offense, special teams.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>They do it all.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>It’s always a great challenge, and that stadiu...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>So, we’re excited for the challenge.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"On Monday, Mahomes added that he praised the ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"For three quarters offensively, we were movin...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\"I went over to the defense and told them: ‘Y’...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>We’ll get to the AFC championship game.\"</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Kansas City Chiefs quarterback Patrick Mahomes...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence     score\n",
       "0                            \\n       Having trouble?  0.000000\n",
       "1                                         Click here.  0.000000\n",
       "2   Kansas City Chiefs quarterback Patrick Mahomes...  0.000000\n",
       "3   Mahomes, surrounded by Travis Kelce, Chris Jon...  0.000000\n",
       "4             They have the Baltimore Ravens up next.  0.000000\n",
       "5   \"Hey, they asked for it, and they got what the...  0.000000\n",
       "6                        \"But he (Andy Reid) said it.  0.000000\n",
       "7                               This s--- ain’t done.  0.000000\n",
       "8        We come back next week ready to f---ing go.\"  0.000000\n",
       "9   CLICK HERE FOR MORE SPORTS COVERAGE ON FOXNEWS...  0.000000\n",
       "10  VIEW THE MOMENT ON X.Reid said it’s not time f...  0.000000\n",
       "11  They still have to get through the Ravens and ...  0.000000\n",
       "12  The Ravens are going to be the Chiefs’ toughes...  0.000000\n",
       "13        Kansas City will have to do it on the road.  0.000000\n",
       "14  The Ravens are hosting a conference championsh...  0.002222\n",
       "15  Baltimore boasts one of the toughest defenses ...  0.000000\n",
       "16  \"There’s no weakness there,\" the star quarterb...  0.000000\n",
       "17               \"It’s going to take our best effort.  0.000000\n",
       "18                   Defense, offense, special teams.  0.000000\n",
       "19                                    They do it all.  0.000000\n",
       "20  It’s always a great challenge, and that stadiu...  0.000000\n",
       "21               So, we’re excited for the challenge.  0.000000\n",
       "22  \"On Monday, Mahomes added that he praised the ...  0.000000\n",
       "23  \"For three quarters offensively, we were movin...  0.000000\n",
       "24  \"I went over to the defense and told them: ‘Y’...  0.000000\n",
       "25           We’ll get to the AFC championship game.\"  0.000000\n",
       "26  Kansas City Chiefs quarterback Patrick Mahomes...  0.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF['score'] = DF['sentence'].apply(impurity)\n",
    "DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n       Having trouble?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\\n       Having trouble?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Click here.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Click here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kansas City Chiefs quarterback Patrick Mahomes...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Kansas City Chiefs quarterback Patrick Mahomes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mahomes, surrounded by Travis Kelce, Chris Jon...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Mahomes, surrounded by Travis Kelce, Chris Jon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They have the Baltimore Ravens up next.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>They have the Baltimore Ravens up next.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Hey, they asked for it, and they got what the...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"Hey, they asked for it, and they got what the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"But he (Andy Reid) said it.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"But he (Andy Reid) said it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This s--- ain’t done.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This s--- ain't done.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We come back next week ready to f---ing go.\"</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>We come back next week ready to f---ing go.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CLICK HERE FOR MORE SPORTS COVERAGE ON FOXNEWS...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>CLICK HERE FOR MORE SPORTS COVERAGE ON FOXNEWS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>VIEW THE MOMENT ON X.Reid said it’s not time f...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>VIEW THE MOMENT ON X.Reid said it's not time f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>They still have to get through the Ravens and ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>They still have to get through the Ravens and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Ravens are going to be the Chiefs’ toughes...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>The Ravens are going to be the Chiefs' toughes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Kansas City will have to do it on the road.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Kansas City will have to do it on the road.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Ravens are hosting a conference championsh...</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>The Ravens are hosting a conference championsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Baltimore boasts one of the toughest defenses ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Baltimore boasts one of the toughest defenses ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"There’s no weakness there,\" the star quarterb...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"There's no weakness there,\" the star quarterb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\"It’s going to take our best effort.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"It's going to take our best effort.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Defense, offense, special teams.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Defense, offense, special teams.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>They do it all.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>They do it all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>It’s always a great challenge, and that stadiu...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>It's always a great challenge, and that stadiu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>So, we’re excited for the challenge.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>So, we're excited for the challenge.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"On Monday, Mahomes added that he praised the ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"On Monday, Mahomes added that he praised the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"For three quarters offensively, we were movin...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"For three quarters offensively, we were movin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\"I went over to the defense and told them: ‘Y’...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"I went over to the defense and told them: 'Y'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>We’ll get to the AFC championship game.\"</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>We'll get to the AFC championship game.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Kansas City Chiefs quarterback Patrick Mahomes...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Kansas City Chiefs quarterback Patrick Mahomes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence     score  \\\n",
       "0                            \\n       Having trouble?  0.000000   \n",
       "1                                         Click here.  0.000000   \n",
       "2   Kansas City Chiefs quarterback Patrick Mahomes...  0.000000   \n",
       "3   Mahomes, surrounded by Travis Kelce, Chris Jon...  0.000000   \n",
       "4             They have the Baltimore Ravens up next.  0.000000   \n",
       "5   \"Hey, they asked for it, and they got what the...  0.000000   \n",
       "6                        \"But he (Andy Reid) said it.  0.000000   \n",
       "7                               This s--- ain’t done.  0.000000   \n",
       "8        We come back next week ready to f---ing go.\"  0.000000   \n",
       "9   CLICK HERE FOR MORE SPORTS COVERAGE ON FOXNEWS...  0.000000   \n",
       "10  VIEW THE MOMENT ON X.Reid said it’s not time f...  0.000000   \n",
       "11  They still have to get through the Ravens and ...  0.000000   \n",
       "12  The Ravens are going to be the Chiefs’ toughes...  0.000000   \n",
       "13        Kansas City will have to do it on the road.  0.000000   \n",
       "14  The Ravens are hosting a conference championsh...  0.002222   \n",
       "15  Baltimore boasts one of the toughest defenses ...  0.000000   \n",
       "16  \"There’s no weakness there,\" the star quarterb...  0.000000   \n",
       "17               \"It’s going to take our best effort.  0.000000   \n",
       "18                   Defense, offense, special teams.  0.000000   \n",
       "19                                    They do it all.  0.000000   \n",
       "20  It’s always a great challenge, and that stadiu...  0.000000   \n",
       "21               So, we’re excited for the challenge.  0.000000   \n",
       "22  \"On Monday, Mahomes added that he praised the ...  0.000000   \n",
       "23  \"For three quarters offensively, we were movin...  0.000000   \n",
       "24  \"I went over to the defense and told them: ‘Y’...  0.000000   \n",
       "25           We’ll get to the AFC championship game.\"  0.000000   \n",
       "26  Kansas City Chiefs quarterback Patrick Mahomes...  0.000000   \n",
       "\n",
       "                                                clean  \n",
       "0                            \\n       Having trouble?  \n",
       "1                                         Click here.  \n",
       "2   Kansas City Chiefs quarterback Patrick Mahomes...  \n",
       "3   Mahomes, surrounded by Travis Kelce, Chris Jon...  \n",
       "4             They have the Baltimore Ravens up next.  \n",
       "5   \"Hey, they asked for it, and they got what the...  \n",
       "6                        \"But he (Andy Reid) said it.  \n",
       "7                               This s--- ain't done.  \n",
       "8        We come back next week ready to f---ing go.\"  \n",
       "9   CLICK HERE FOR MORE SPORTS COVERAGE ON FOXNEWS...  \n",
       "10  VIEW THE MOMENT ON X.Reid said it's not time f...  \n",
       "11  They still have to get through the Ravens and ...  \n",
       "12  The Ravens are going to be the Chiefs' toughes...  \n",
       "13        Kansas City will have to do it on the road.  \n",
       "14  The Ravens are hosting a conference championsh...  \n",
       "15  Baltimore boasts one of the toughest defenses ...  \n",
       "16  \"There's no weakness there,\" the star quarterb...  \n",
       "17               \"It's going to take our best effort.  \n",
       "18                   Defense, offense, special teams.  \n",
       "19                                    They do it all.  \n",
       "20  It's always a great challenge, and that stadiu...  \n",
       "21               So, we're excited for the challenge.  \n",
       "22  \"On Monday, Mahomes added that he praised the ...  \n",
       "23  \"For three quarters offensively, we were movin...  \n",
       "24  \"I went over to the defense and told them: 'Y'...  \n",
       "25           We'll get to the AFC championship game.\"  \n",
       "26  Kansas City Chiefs quarterback Patrick Mahomes...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF['clean'] = DF['sentence'].apply(normalize)\n",
    "DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sunday\n",
      "\n",
      "\n",
      "sundae\n",
      "{'sundry', 'sundae'}\n",
      "jan.\n",
      "\n",
      "\n",
      "jane\n",
      "{'jane'}\n",
      "``\n",
      "\n",
      "\n",
      "i\n",
      "{'mu', 'kw', 'my', 'up', 'si', 'at', 'to', 'h', 'pa', 'on', 'u', 'bi', 'ta', 't', 'ne', 'ls', 'uh', 'dm', 'ha', 'g', 'he', 'hg', 'fa', 's', 'pe', 'us', 'om', 'ps', 'y', 'w', 'c', 'r', 'me', 'er', 'no', 'of', 'el', 'jg', 'pi', 'la', 'jo', 'cw', 'vs', 'is', 'lo', 'oo', 'an', 'id', 'em', 'um', 'de', 'ox', 'if', 'wo', 'd', 'dc', 'hl', 'ho', 'k', 'a', 'lm', 'xi', 're', 'eh', 'mi', 'mf', 'gs', 'ka', 'ki', 'ln', 'am', 'o', 'f', 'n', 'ti', 'ai', 'j', 'it', 'ax', 'ex', 'ut', 'ss', 'hm', 'ay', 'z', 'v', 'en', 'mb', 'b', 'cs', 'be', 'p', 'xu', 'by', 'lx', 'ja', 'li', 'yo', 'hi', 'et', 'es', 'qi', 'kb', 'do', 'ms', 'i', 'nu', 'e', 'od', 'so', 'sh', 'or', 'db', 'ts', 'kc', 'as', 'm', 'l', 'oh', 'go', 'ad', 'ye', 'dg', 'x', 'in', 'ma', 'ow', 'rs', 'ah', 'oi', 'q', 'ya', 'we', 'ks'}\n",
      "night.chargers\n",
      "\n",
      "\n",
      "None\n",
      "None\n",
      "jackson\n",
      "\n",
      "\n",
      "jacks\n",
      "{'jackpot', 'jacks'}\n",
      "n't\n",
      "\n",
      "\n",
      "not\n",
      "{'net', 'not', \"an't\", 'nut', 'nit'}\n",
      "kraus\n",
      "\n",
      "\n",
      "krauts\n",
      "{'kraut', 'krauts'}\n",
      "jones\n",
      "\n",
      "\n",
      "ones\n",
      "{'cones', 'pones', 'hones', 'zones', 'bones', 'sones', 'jokes', 'ones', 'tones', 'nones'}\n",
      "n.y.\n",
      "\n",
      "\n",
      "nay\n",
      "{'nays', 'nay'}\n",
      "ii\n",
      "\n",
      "\n",
      "i\n",
      "{'i', 'ti', 'bi', 'ai', 'li', 'it', 'in', 'hi', 'oi', 'is', 'if', 'si', 'xi', 'qi', 'ki', 'pi', 'id', 'mi'}\n",
      "andy\n",
      "\n",
      "\n",
      "and\n",
      "{'any', 'handy', 'randy', 'sandy', 'dandy', 'candy', 'bandy', 'and'}\n",
      "x.reid\n",
      "\n",
      "\n",
      "nereid\n",
      "{'nereid'}\n",
      "baltimore\n",
      "\n",
      "\n",
      "None\n",
      "None\n",
      "photo/adrian\n",
      "\n",
      "\n",
      "None\n",
      "None\n",
      "''\n",
      "\n",
      "\n",
      "i\n",
      "{'mu', 'my', 'kw', 'at', 'up', 'si', 'to', 'h', 'pa', 'on', 'u', 'bi', 'ta', 't', 'ne', 'ls', 'uh', \"h'm\", 'dm', 'ha', 'g', 'fa', 'he', 'hg', 'c', 'pe', 'us', 'om', 'ps', 'y', 'w', 's', 'r', 'me', 'er', 'no', 'of', 'el', 'jg', 'la', 'pi', 'jo', 'cw', 'vs', 'is', 'lo', 'oo', 'an', 'id', 'em', 'um', 'de', 'ox', 'if', 'wo', 'd', 'dc', 'hl', 'ho', 'k', 'a', 'lm', 'xi', 're', 'eh', 'mi', 'mf', 'gs', 'ka', 'ki', 'ln', 'am', 'o', 'f', 'n', 'ti', 'ai', 'it', 'j', 'ax', 'ex', 'ut', 'ss', 'hm', 'ay', 'z', 'v', 'en', 'mb', 'b', 'cs', 'be', 'p', 'xu', 'by', 'lx', 'ja', 'li', 'yo', 'hi', 'et', 'es', 'qi', 'kb', 'do', 'ms', 'i', 'nu', 'e', 'od', 'so', 'sh', 'or', 'db', 'ts', 'kc', 'as', 'm', 'oh', 'l', 'ad', 'go', 'ye', 'dg', 'x', 'in', 'ma', 'ow', 'rs', 'ah', 'oi', 'q', 'ya', 'we', 'ks'}\n",
      "ap\n",
      "\n",
      "\n",
      "a\n",
      "{'gap', 'pap', 'bap', 'ai', 'dap', 'ax', 'up', 'at', 'lap', 'pa', 'an', 'ay', 'as', 'zap', 'hap', 'tap', 'cap', 'yap', 'asp', 'ape', 'ad', 'alp', 'amp', 'a', 'p', 'ah', 'map', 'apt', 'nap', 'sap', 'rap', 'am'}\n",
      "-ing\n",
      "\n",
      "\n",
      "sing\n",
      "{'ding', 'sing', 'wing', 'king', 'ting', 'ring', 'ping', 'zing'}\n",
      "reid\n",
      "\n",
      "\n",
      "read\n",
      "{'raid', 'read', 'redd', 'rein', 'rid', 'reed', 'rend', 'redid', 'red', 'rebid', 'reis'}\n",
      "monday\n",
      "\n",
      "\n",
      "money\n",
      "{'monad', 'moray', 'monkey', 'monas', 'fondly', 'moody', 'midday', 'modal', 'monads', 'moldy', 'noonday', 'today', 'mayday', 'monody', 'monde', 'money'}\n",
      "jim\n",
      "\n",
      "\n",
      "him\n",
      "{'jib', 'jig', 'rim', 'jam', 'dim', 'aim', 'nim', 'mim', 'vim', 'him'}\n",
      "kansas\n",
      "\n",
      "\n",
      "canvas\n",
      "{'ganjas', 'canvas', 'vandas', 'kwanzas', 'anas', 'kanzus', 'kavas', 'annas', 'balsas', 'manias', 'manses', 'paisas', 'kinas', 'pandas', 'mantas', 'pangas', 'salsas', 'kappas', 'tankas', 'kashas', 'anoas', 'cannas', 'kantar', 'kana', 'manas'}\n",
      "'re\n",
      "\n",
      "\n",
      "are\n",
      "{'are', 're', 'ore', 'ire', 'ere'}\n",
      "brittany\n",
      "\n",
      "\n",
      "dittany\n",
      "{'dittany'}\n",
      "patrick\n",
      "\n",
      "\n",
      "trick\n",
      "{'pathic', 'prick', 'patriot', 'iatric', 'hayrick', 'trick', 'strick'}\n",
      "kelce\n",
      "\n",
      "\n",
      "else\n",
      "{'ketch', 'deice', 'delve', 'dele', 'pence', 'kedge', 'else', 'terce', 'keel', 'kerne', 'veloce', 'welch', 'belch', 'kelpie', 'kyle', 'helve', 'fence', 'kelter', 'kale', 'kente', 'kelt', 'deuce', 'kelp', 'keck', 'hence', 'dolce', 'peace', 'belle', 'belie', 'recce', 'kelly'}\n",
      "afc\n",
      "\n",
      "\n",
      "aft\n",
      "{'aft', 'arc'}\n",
      "'s\n",
      "\n",
      "\n",
      "is\n",
      "{'ts', 'as', 'gs', 'vs', 'is', 'ps', 'es', 'rs', 'cs', 's', 'ls', 'us', 'ks', 'ms', 'ss'}\n",
      "'ll\n",
      "\n",
      "\n",
      "all\n",
      "{'ell', 'all', 'ill'}\n",
      "nfl\n",
      "\n",
      "\n",
      "nil\n",
      "{'nil'}\n",
      "mahomes\n",
      "\n",
      "\n",
      "homes\n",
      "{'mahouts', 'manholes', 'radomes', 'homes'}\n",
      "photo/frank\n",
      "\n",
      "\n",
      "None\n",
      "None\n",
      "lamar\n",
      "\n",
      "\n",
      "lama\n",
      "{'damar', 'lamer', 'lama', 'lamas', 'lazar'}\n",
      "mvp\n",
      "\n",
      "\n",
      "map\n",
      "{'map', 'mop'}\n",
      "chris\n",
      "\n",
      "\n",
      "chis\n",
      "{'chrism', 'chis'}\n",
      "travis\n",
      "\n",
      "\n",
      "traves\n",
      "{'traves'}\n",
      "foxnews.com\n",
      "\n",
      "\n",
      "None\n",
      "None\n",
      "--\n",
      "\n",
      "\n",
      "i\n",
      "{'mu', 'kw', 'my', 'up', 'at', 'si', 'to', 'h', 'pa', 'on', 'u', 'bi', 'ta', 't', 'ne', 'ls', 'uh', 'dm', 'ha', 'g', 'fa', 'he', 'hg', 's', 'c', 'us', 'om', 'ps', 'pe', 'w', 'y', 'r', 'me', 'er', 'no', 'of', 'el', 'jg', 'la', 'pi', 'jo', 'cw', 'vs', 'is', 'lo', 'oo', 'an', 'id', 'em', 'um', 'de', 'ox', 'if', 'wo', 'd', 'dc', 'hl', 'ho', 'k', 'a', 'lm', 'xi', 're', 'eh', 'mi', 'mf', 'gs', 'ka', 'ki', 'ln', 'am', 'o', 'f', 'n', 'ti', 'ai', 'j', 'it', 'ax', 'ex', 'ut', 'ss', 'hm', 'ay', 'z', 'v', 'en', 'mb', 'b', 'cs', 'be', 'p', 'xu', 'by', 'lx', 'ja', 'li', 'yo', 'hi', 'et', 'es', 'qi', 'kb', 'do', 'ms', 'i', 'nu', 'e', 'od', 'so', 'sh', 'or', 'db', 'ts', 'kc', 'as', 'm', 'oh', 'ad', 'l', 'go', 'ye', 'dg', 'x', 'in', 'ma', 'ow', 'rs', 'ah', 'oi', 'q', 'ya', 'we', 'ks'}\n",
      "harbaugh\n",
      "\n",
      "\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# find all the unique tokens\n",
    "# set is find unique\n",
    "# nltk.word_tokenize is break down into words\n",
    "# \" \".join is combine into one long text\n",
    "# .to_list() is a function to convert to list \n",
    "clean_tokens = set(nltk.word_tokenize(\" \".join(DF['clean'].to_list())))\n",
    "# what is wrong? \n",
    "misspelled = spell.unknown(clean_tokens)\n",
    "\n",
    "for word in misspelled:\n",
    "    #  what's the word\n",
    "    print(word)\n",
    "    print(\"\\n\")\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # Get a list of `likely` options\n",
    "    print(spell.candidates(word))\n",
    "    \n",
    "# make a dictionary of the misspelled word and the correction\n",
    "# use find and replace in re to fix them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Having', 'trouble'],\n",
       " ['Click'],\n",
       " ['Kansas',\n",
       "  'City',\n",
       "  'Chiefs',\n",
       "  'quarterback',\n",
       "  'Patrick',\n",
       "  'Mahomes',\n",
       "  'delivered',\n",
       "  'epic',\n",
       "  'pep',\n",
       "  'talk',\n",
       "  'teammates',\n",
       "  'team',\n",
       "  'defeated',\n",
       "  'Buffalo',\n",
       "  'Bills',\n",
       "  'advance',\n",
       "  'AFC',\n",
       "  'championship',\n",
       "  'game',\n",
       "  'Sunday'],\n",
       " ['Mahomes',\n",
       "  'surrounded',\n",
       "  'Travis',\n",
       "  'Kelce',\n",
       "  'Chris',\n",
       "  'Jones',\n",
       "  'agreed',\n",
       "  'coach',\n",
       "  'Andy',\n",
       "  'Reid',\n",
       "  'job'],\n",
       " ['Baltimore', 'Ravens'],\n",
       " ['Hey', 'asked', 'got', 'asked', 'Mahomes', 'said'],\n",
       " ['Andy', 'Reid', 'said'],\n",
       " ['s---', 'ai'],\n",
       " ['come', 'week', 'ready', 'f', 'ing'],\n",
       " ['CLICK',\n",
       "  'SPORTS',\n",
       "  'COVERAGE',\n",
       "  'FOXNEWS.COM',\n",
       "  'Kansas',\n",
       "  'City',\n",
       "  'Chiefs',\n",
       "  'quarterback',\n",
       "  'Patrick',\n",
       "  'Mahomes',\n",
       "  'reacts',\n",
       "  'beating',\n",
       "  'Buffalo',\n",
       "  'Bills',\n",
       "  'AFC',\n",
       "  'divisional',\n",
       "  'playoff',\n",
       "  'game',\n",
       "  'Sunday',\n",
       "  'Jan.',\n",
       "  'Orchard',\n",
       "  'Park',\n",
       "  'N.Y.',\n",
       "  'AP',\n",
       "  'Photo',\n",
       "  'Adrian',\n",
       "  'Kraus',\n",
       "  'Kelce',\n",
       "  'added',\n",
       "  'case',\n",
       "  'know',\n",
       "  'got',\n",
       "  'week']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = []\n",
    "\n",
    "# only the tagger and lemmatizer\n",
    "for doc in nlp.pipe(DF['clean'].tolist(), disable=[\"tok2vec\", \"ner\", \"parser\"]):\n",
    "  tokens = textacy.extract.words(doc,\n",
    "            filter_stops = True,           # default True, no stopwords\n",
    "            filter_punct = True,           # default True, no punctuation\n",
    "            filter_nums = True,            # default False, no numbers\n",
    "            include_pos = None,            # default None = include all\n",
    "            exclude_pos = None,            # default None = exclude none\n",
    "            min_freq = 1)                  # minimum frequency of words\n",
    "  output.append([str(word) for word in tokens]) # close output append \n",
    "\n",
    "output[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Mahomes': 9,\n",
       "         'Chiefs': 6,\n",
       "         'game': 6,\n",
       "         'Kansas': 5,\n",
       "         'City': 5,\n",
       "         'quarterback': 5,\n",
       "         'Bills': 5,\n",
       "         'AFC': 5,\n",
       "         'Sunday': 5,\n",
       "         'said': 5,\n",
       "         'Patrick': 4,\n",
       "         'Buffalo': 4,\n",
       "         'Ravens': 4,\n",
       "         'championship': 3,\n",
       "         'got': 3,\n",
       "         'divisional': 3,\n",
       "         'playoff': 3,\n",
       "         'Jan.': 3,\n",
       "         'Orchard': 3,\n",
       "         'Park': 3,\n",
       "         'N.Y.': 3,\n",
       "         'AP': 3,\n",
       "         'Photo': 3,\n",
       "         'going': 3,\n",
       "         'Kelce': 2,\n",
       "         'Andy': 2,\n",
       "         'Reid': 2,\n",
       "         'Baltimore': 2,\n",
       "         'asked': 2,\n",
       "         'week': 2,\n",
       "         'Adrian': 2,\n",
       "         'Kraus': 2,\n",
       "         'added': 2,\n",
       "         'time': 2,\n",
       "         'win': 2,\n",
       "         'toughest': 2,\n",
       "         'NFL': 2,\n",
       "         'offense': 2,\n",
       "         'challenge': 2,\n",
       "         'defense': 2,\n",
       "         'shut': 2,\n",
       "         'quarter': 2,\n",
       "         'Having': 1,\n",
       "         'trouble': 1,\n",
       "         'Click': 1,\n",
       "         'delivered': 1,\n",
       "         'epic': 1,\n",
       "         'pep': 1,\n",
       "         'talk': 1,\n",
       "         'teammates': 1,\n",
       "         'team': 1,\n",
       "         'defeated': 1,\n",
       "         'advance': 1,\n",
       "         'surrounded': 1,\n",
       "         'Travis': 1,\n",
       "         'Chris': 1,\n",
       "         'Jones': 1,\n",
       "         'agreed': 1,\n",
       "         'coach': 1,\n",
       "         'job': 1,\n",
       "         'Hey': 1,\n",
       "         's---': 1,\n",
       "         'ai': 1,\n",
       "         'come': 1,\n",
       "         'ready': 1,\n",
       "         'f': 1,\n",
       "         'ing': 1,\n",
       "         'CLICK': 1,\n",
       "         'SPORTS': 1,\n",
       "         'COVERAGE': 1,\n",
       "         'FOXNEWS.COM': 1,\n",
       "         'reacts': 1,\n",
       "         'beating': 1,\n",
       "         'case': 1,\n",
       "         'know': 1,\n",
       "         'VIEW': 1,\n",
       "         'MOMENT': 1,\n",
       "         'X.Reid': 1,\n",
       "         'dancing': 1,\n",
       "         'Super': 1,\n",
       "         'Bowl': 1,\n",
       "         'celebrations': 1,\n",
       "         'happen': 1,\n",
       "         'matchup': 1,\n",
       "         'road': 1,\n",
       "         'hosting': 1,\n",
       "         'conference': 1,\n",
       "         'history': 1,\n",
       "         'M&T': 1,\n",
       "         'Bank': 1,\n",
       "         'Stadium': 1,\n",
       "         'night': 1,\n",
       "         'CHARGERS': 1,\n",
       "         'PURSUIT': 1,\n",
       "         'JIM': 1,\n",
       "         'HARBAUGH': 1,\n",
       "         'ADVANCING': 1,\n",
       "         'FINAL': 1,\n",
       "         'STAGES': 1,\n",
       "         'REPORT': 1,\n",
       "         'Brittany': 1,\n",
       "         'right': 1,\n",
       "         'hugs': 1,\n",
       "         'knows': 1,\n",
       "         'work': 1,\n",
       "         'cut': 1,\n",
       "         'boasts': 1,\n",
       "         'defenses': 1,\n",
       "         'likely': 1,\n",
       "         'MVP': 1,\n",
       "         'Lamar': 1,\n",
       "         'Jackson': 1,\n",
       "         'weakness': 1,\n",
       "         'star': 1,\n",
       "         'best': 1,\n",
       "         'effort': 1,\n",
       "         'Defense': 1,\n",
       "         'special': 1,\n",
       "         'teams': 1,\n",
       "         'great': 1,\n",
       "         'stadium': 1,\n",
       "         'rocking': 1,\n",
       "         'excited': 1,\n",
       "         'Monday': 1,\n",
       "         'praised': 1,\n",
       "         'coming': 1,\n",
       "         'clutch': 1,\n",
       "         'quarters': 1,\n",
       "         'offensively': 1,\n",
       "         'moving': 1,\n",
       "         'ball': 1,\n",
       "         'field': 1,\n",
       "         'went': 1,\n",
       "         'told': 1,\n",
       "         \"Y'all\": 1,\n",
       "         'football': 1,\n",
       "         'rolls': 1,\n",
       "         'pocket': 1,\n",
       "         'Frank': 1,\n",
       "         'Franklin': 1,\n",
       "         'II': 1})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all items\n",
    "type(output)\n",
    "# first list\n",
    "type(output[0])\n",
    "# first list, first item (this is the issue!)\n",
    "type(output[0][0])\n",
    "\n",
    "Counter(chain.from_iterable(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 10.5 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 1.0/12.8 MB 11.0 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 10.8 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.1/12.8 MB 11.1 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 2.6/12.8 MB 11.1 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 3.1/12.8 MB 10.9 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 11.1 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 4.1/12.8 MB 11.0 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 4.7/12.8 MB 11.5 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 11.5 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 11.4 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.3/12.8 MB 11.5 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.8/12.8 MB 11.5 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.4/12.8 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.9/12.8 MB 11.5 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.4/12.8 MB 11.5 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.0/12.8 MB 11.5 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 10.1/12.8 MB 11.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.6/12.8 MB 11.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 11.1/12.8 MB 11.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 11.7/12.8 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.1/12.8 MB 11.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 11.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 11.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\user7\\appdata\\roaming\\python\\python39\\site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\user7\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user7\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user7\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user7\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user7\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\user7\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\user7\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\user7\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user7\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\user7\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\user7\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\user7\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (58.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user7\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\program files\\python39\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\program files\\python39\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\program files\\python39\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\user7\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\user7\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python39\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\program files\\python39\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\user7\\appdata\\roaming\\python\\python39\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python39\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "\u001b[38;5;2mвњ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# nltk.download(\"punkt\")\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "# only for datalore \n",
    "import subprocess\n",
    "#%%\n",
    "print(subprocess.getoutput(\"python -m spacy download en_core_web_sm\"))\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import textacy\n",
    "import summa\n",
    "from summa import keywords\n",
    "\n",
    "from snorkel.preprocess import preprocessor\n",
    "from snorkel.types import DataPoint\n",
    "from itertools import combinations\n",
    "from snorkel.labeling import labeling_function\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'The_Shadow_Over_Innsmouth.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# creating a pdf file object\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pdfFileObj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mThe_Shadow_Over_Innsmouth.pdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# creating a pdf reader object\u001b[39;00m\n\u001b[0;32m      5\u001b[0m pdfReader \u001b[38;5;241m=\u001b[39m PyPDF2\u001b[38;5;241m.\u001b[39mPdfReader(pdfFileObj)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'The_Shadow_Over_Innsmouth.pdf'"
     ]
    }
   ],
   "source": [
    "# creating a pdf file object\n",
    "pdfFileObj = open('The_Shadow_Over_Innsmouth.pdf', 'rb')\n",
    "  \n",
    "# creating a pdf reader object\n",
    "pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
    "\n",
    "# how many pages\n",
    "len(pdfReader.pages)\n",
    "\n",
    "# creating a page object\n",
    "pageObj = pdfReader.pages\n",
    "  \n",
    "# extracting text from page\n",
    "# loop here to get it all \n",
    "text = []\n",
    "for page in pageObj:\n",
    "  text.append(page.extract_text())\n",
    "\n",
    "# closing the pdf file object\n",
    "pdfFileObj.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m saved_words \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# loop over each word\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mword_tokenize(book):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# if the word starts with a number and ends with a letter\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^[0-9].*[a-zA-Z]$\u001b[39m\u001b[38;5;124m'\u001b[39m, word) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m): \n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m# take out the numbers and save into our text\u001b[39;00m\n\u001b[0;32m      9\u001b[0m         saved_words\u001b[38;5;241m.\u001b[39mappend(re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[0-9]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, word))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# create a place to save the text\n",
    "saved_words = []\n",
    "\n",
    "# loop over each word\n",
    "for word in nltk.word_tokenize(book):\n",
    "    # if the word starts with a number and ends with a letter\n",
    "    if (re.search(r'^[0-9].*[a-zA-Z]$', word) != \"None\"): \n",
    "        # take out the numbers and save into our text\n",
    "        saved_words.append(re.sub(r'[0-9]', '', word))\n",
    "    # if not then save just the word \n",
    "    else:\n",
    "        saved_words.append(word)\n",
    "book = ' '.join(saved_words)\n",
    "DF = pd.DataFrame(\n",
    "    nltk.sent_tokenize(book),\n",
    "    columns = [\"sentences\"]\n",
    ")\n",
    "\n",
    "DF.head()\n",
    "\n",
    "# for IE, we want sentence and/or paragraph level structure \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# easier to loop over the big text file than loop over words AND rows in pandas \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m spacy_pos_tagged \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mstr\u001b[39m(word), word\u001b[38;5;241m.\u001b[39mtag_, word\u001b[38;5;241m.\u001b[39mpos_) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnlp\u001b[49m(book)]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# each row represents one token \u001b[39;00m\n\u001b[0;32m      4\u001b[0m DF_POS \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m      5\u001b[0m     spacy_pos_tagged,\n\u001b[0;32m      6\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecific_tag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupos\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "# easier to loop over the big text file than loop over words AND rows in pandas \n",
    "spacy_pos_tagged = [(str(word), word.tag_, word.pos_) for word in nlp(book)]\n",
    "# each row represents one token \n",
    "DF_POS = pd.DataFrame(\n",
    "    spacy_pos_tagged,\n",
    "    columns = [\"token\", \"specific_tag\", \"upos\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DF_POS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mDF_POS\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupos\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DF_POS' is not defined"
     ]
    }
   ],
   "source": [
    "DF_POS['upos'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m DF_POS2 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mcrosstab(DF_POS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m], DF_POS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupos\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# convert to true false to add up how many times not zero \u001b[39;00m\n\u001b[0;32m      3\u001b[0m DF_POS2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m DF_POS2\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "DF_POS2 = pd.crosstab(DF_POS['token'], DF_POS['upos'])\n",
    "# convert to true false to add up how many times not zero \n",
    "DF_POS2['total'] = DF_POS2.astype(bool).sum(axis=1)\n",
    "#print out the rows that aren't 1 \n",
    "DF_POS2[DF_POS2['total'] > 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'textacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# textacy KPE\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# build an english language for textacy pipe\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m en \u001b[38;5;241m=\u001b[39m \u001b[43mtextacy\u001b[49m\u001b[38;5;241m.\u001b[39mload_spacy_lang(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# build a processor for textacy using spacy and process text\u001b[39;00m\n\u001b[0;32m      6\u001b[0m doc \u001b[38;5;241m=\u001b[39m textacy\u001b[38;5;241m.\u001b[39mmake_spacy_doc(book, lang \u001b[38;5;241m=\u001b[39m en)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'textacy' is not defined"
     ]
    }
   ],
   "source": [
    "# textacy KPE\n",
    "# build an english language for textacy pipe\n",
    "en = textacy.load_spacy_lang(\"en_core_web_sm\", disable=(\"parser\"))\n",
    "\n",
    "# build a processor for textacy using spacy and process text\n",
    "doc = textacy.make_spacy_doc(book, lang = en)\n",
    "\n",
    "# text rank algorithm  \n",
    "print([kps for kps, weights in textacy.extract.keyterms.textrank(doc, normalize = \"lemma\",  topn = 5)])\n",
    "\n",
    "terms = set([term for term, weight in textacy.extract.keyterms.textrank(doc)])\n",
    "print(textacy.extract.utils.aggregate_term_variants(terms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TR_keywords = keywords.keywords(book, scores = True)\n",
    "#print(TR_keywords[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# easier to loop over the big text file than loop over words AND rows in pandas \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m spacy_ner_tagged \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mstr\u001b[39m(word\u001b[38;5;241m.\u001b[39mtext), word\u001b[38;5;241m.\u001b[39mlabel_) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnlp\u001b[49m(book)\u001b[38;5;241m.\u001b[39ments]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# each row represents one token \u001b[39;00m\n\u001b[0;32m      5\u001b[0m DF_NER \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m      6\u001b[0m     spacy_ner_tagged,\n\u001b[0;32m      7\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "# easier to loop over the big text file than loop over words AND rows in pandas \n",
    "spacy_ner_tagged = [(str(word.text), word.label_) for word in nlp(book).ents]\n",
    "\n",
    "# each row represents one token \n",
    "DF_NER = pd.DataFrame(\n",
    "    spacy_ner_tagged,\n",
    "    columns = [\"token\", \"entity\"]\n",
    ")\n",
    "print(DF_NER['entity'].value_counts())\n",
    "\n",
    "DF_NER2 = pd.crosstab(DF_NER['token'], DF_NER['entity'])\n",
    "print(DF_NER2)\n",
    "# convert to true false to add up how many times not zero \n",
    "DF_NER2['total'] = DF_NER2.astype(bool).sum(axis=1)\n",
    "#print out the rows that aren't 1 \n",
    "DF_NER2[DF_NER2['total'] > 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 50\u001b[0m\n\u001b[0;32m     40\u001b[0m       \u001b[38;5;66;03m# store all this in a list \u001b[39;00m\n\u001b[0;32m     41\u001b[0m       stored_entities\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m     42\u001b[0m         [x, \u001b[38;5;66;03m# original text\u001b[39;00m\n\u001b[0;32m     43\u001b[0m         tokens, \u001b[38;5;66;03m# tokens\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m         person2_ids2 \u001b[38;5;66;03m# person 2 id token tuple\u001b[39;00m\n\u001b[0;32m     48\u001b[0m         ])\n\u001b[1;32m---> 50\u001b[0m \u001b[43mDF\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(get_entities)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# create dataframe in snorkel structure \u001b[39;00m\n\u001b[0;32m     53\u001b[0m DF_dev \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(stored_entities, columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson1_word_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson2_word_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DF' is not defined"
     ]
    }
   ],
   "source": [
    "stored_entities = []\n",
    "\n",
    "# first get the entities, must be two for relationship matches\n",
    "def get_entities(x):\n",
    "  \"\"\"\n",
    "  Grabs the names using spacy's entity labeler\n",
    "  \"\"\"\n",
    "  # get all the entities in this row \n",
    "  processed = nlp(x)\n",
    "  # get the tokens for each sentence\n",
    "  tokens = [word.text for word in processed]\n",
    "  # get all the entities - notice this is only for persons \n",
    "  temp = [(str(ent), ent.label_) for ent in processed.ents if ent.label_ != \"\"]\n",
    "  # only move on if this row has at least two\n",
    "  if len(temp) > 1: \n",
    "    # finds all the combinations of pairs \n",
    "    temp2 = list(combinations(temp, 2))\n",
    "    # for each pair combination \n",
    "    for (person1, person2) in temp2:\n",
    "      # find the names in the person 1\n",
    "      person1_words = [word.text for word in nlp(person1[0])]\n",
    "      # find the token numbers for person 1\n",
    "      person1_ids = [i for i, val in enumerate(tokens) if val in person1_words]\n",
    "      # output in (start, stop) token tuple format \n",
    "      if len(person1_words) > 1:\n",
    "        person1_ids2 = tuple(idx for idx in person1_ids[0:2])\n",
    "      else:\n",
    "        id_1 = [idx for idx in person1_ids]\n",
    "        person1_ids2 = (id_1[0], id_1[0])\n",
    "        \n",
    "      # do the same thing with person 2\n",
    "      person2_words = [word.text for word in nlp(person2[0])]\n",
    "      person2_ids = [i for i, val in enumerate(tokens) if val in person2_words[0:2]]\n",
    "      if len(person2_words) > 1:\n",
    "        person2_ids2 = tuple(idx for idx in person2_ids)\n",
    "      else:\n",
    "        id_2 = [idx for idx in person2_ids[0:2]]\n",
    "        person2_ids2 = (id_2[0], id_2[0])  \n",
    "      \n",
    "      # store all this in a list \n",
    "      stored_entities.append(\n",
    "        [x, # original text\n",
    "        tokens, # tokens\n",
    "        person1[0], # person 1 name\n",
    "        person2[0], # person 2 name\n",
    "        person1_ids2, # person 1 id token tuple \n",
    "        person2_ids2 # person 2 id token tuple\n",
    "        ])\n",
    "\n",
    "DF['sentences'].apply(get_entities)\n",
    "\n",
    "# create dataframe in snorkel structure \n",
    "DF_dev = pd.DataFrame(stored_entities, columns = [\"sentence\", \"tokens\", \"person1\", \"person2\", \"person1_word_idx\", \"person2_word_idx\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# live locate home road roads in at street (locations tied together)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# family terms for people \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# get words between the data points \u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;129m@preprocessor\u001b[39m()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_text_between\u001b[39m(cand: DataPoint) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataPoint:\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    Returns the text between the two person mentions in the sentence\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     start \u001b[38;5;241m=\u001b[39m cand\u001b[38;5;241m.\u001b[39mperson1_word_idx[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "# live locate home road roads in at street (locations tied together)\n",
    "# family terms for people \n",
    "\n",
    "# get words between the data points \n",
    "@preprocessor()\n",
    "def get_text_between(cand: DataPoint) -> DataPoint:\n",
    "    \"\"\"\n",
    "    Returns the text between the two person mentions in the sentence\n",
    "    \"\"\"\n",
    "    start = cand.person1_word_idx[1] + 1\n",
    "    end = cand.person2_word_idx[0]\n",
    "    cand.between_tokens = cand.tokens[start:end]\n",
    "    return cand\n",
    "\n",
    "# get words next to the data points\n",
    "@preprocessor()\n",
    "def get_left_tokens(cand: DataPoint) -> DataPoint:\n",
    "    \"\"\"\n",
    "    Returns tokens in the length 3 window to the left of the person mentions\n",
    "    \"\"\"\n",
    "    # TODO: need to pass window as input params\n",
    "    window = 5\n",
    "\n",
    "    end = cand.person1_word_idx[0]\n",
    "    cand.person1_left_tokens = cand.tokens[0:end][-1 - window : -1]\n",
    "\n",
    "    end = cand.person2_word_idx[0]\n",
    "    cand.person2_left_tokens = cand.tokens[0:end][-1 - window : -1]\n",
    "    return cand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labeling_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m ABSTAIN \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      8\u001b[0m location \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlive\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliving\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocated\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhome\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroads\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstreet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstreets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;129m@labeling_function\u001b[39m(resources\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(location\u001b[38;5;241m=\u001b[39mlocation), pre\u001b[38;5;241m=\u001b[39m[get_text_between])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbetween_location\u001b[39m(x, location):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m found_location \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(location\u001b[38;5;241m.\u001b[39mintersection(\u001b[38;5;28mset\u001b[39m(x\u001b[38;5;241m.\u001b[39mbetween_tokens))) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m ABSTAIN\n\u001b[0;32m     14\u001b[0m \u001b[38;5;129m@labeling_function\u001b[39m(resources\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(location\u001b[38;5;241m=\u001b[39mlocation), pre\u001b[38;5;241m=\u001b[39m[get_left_tokens])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mleft_location\u001b[39m(x, location):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'labeling_function' is not defined"
     ]
    }
   ],
   "source": [
    "# live locate home road roads in at street (locations tied together)\n",
    "# family terms for people \n",
    "\n",
    "found_location = 1\n",
    "found_family = -1\n",
    "ABSTAIN = 0\n",
    "\n",
    "location = {\"live\", \"living\", \"locate\", \"located\", \"home\", \"road\", \"roads\", \"street\", \"streets\", \"in\", \"at\", \"of\"}\n",
    "\n",
    "@labeling_function(resources=dict(location=location), pre=[get_text_between])\n",
    "def between_location(x, location):\n",
    "    return found_location if len(location.intersection(set(x.between_tokens))) > 0 else ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(location=location), pre=[get_left_tokens])\n",
    "def left_location(x, location):\n",
    "    if len(set(location).intersection(set(x.person1_left_tokens))) > 0:\n",
    "        return found_location\n",
    "    elif len(set(location).intersection(set(x.person2_left_tokens))) > 0:\n",
    "        return found_location\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "family = {\"spouse\", \"wife\", \"husband\", \"ex-wife\", \"ex-husband\", \"marry\", \n",
    "          \"married\", \"father\", \"mother\", \"sister\", \"brother\", \"son\", \"daughter\", \n",
    "          \"grandfather\", \"grandmother\", \"uncle\", \"aunt\", \"cousin\", \n",
    "          \"boyfriend\", \"girlfriend\"}\n",
    "\n",
    "@labeling_function(resources=dict(family=family), pre=[get_text_between])\n",
    "def between_family(x, family):\n",
    "    return found_family if len(family.intersection(set(x.between_tokens))) > 0 else ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(family=family), pre=[get_left_tokens])\n",
    "def left_family(x, family):\n",
    "    if len(set(family).intersection(set(x.person1_left_tokens))) > 0:\n",
    "        return found_family\n",
    "    elif len(set(family).intersection(set(x.person2_left_tokens))) > 0:\n",
    "        return found_family\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "# create a list of functions to run     \n",
    "lfs = [\n",
    "    between_location,\n",
    "    left_location,\n",
    "    between_family,\n",
    "    left_family\n",
    "]\n",
    "# build the applier function \n",
    "applier = PandasLFApplier(lfs)\n",
    "# run it on the dataset \n",
    "L_dev = applier.apply(DF_dev)\n",
    "L_dev\n",
    "DF_combined = pd.concat([DF_dev, pd.DataFrame(L_dev, columns = [\"location1\", \"location2\", \"family1\", \"family2\"])], axis = 1)\n",
    "DF_combined\n",
    "\n",
    "DF_combined['location_yes'] = DF_combined['location1'] + DF_combined[\"location2\"]\n",
    "DF_combined['family_yes'] = DF_combined['family1'] + DF_combined[\"family2\"]\n",
    "\n",
    "print(DF_combined['location_yes'].value_counts())\n",
    "print(DF_combined['family_yes'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DF_combined' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# locations only\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m DF_loc \u001b[38;5;241m=\u001b[39m \u001b[43mDF_combined\u001b[49m[DF_combined[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation_yes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m DF_loc \u001b[38;5;241m=\u001b[39m DF_loc[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperson1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperson2\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mreset_index(drop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m cooc_loc \u001b[38;5;241m=\u001b[39m DF_loc\u001b[38;5;241m.\u001b[39mgroupby(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson2\u001b[39m\u001b[38;5;124m\"\u001b[39m], as_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39msize()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DF_combined' is not defined"
     ]
    }
   ],
   "source": [
    "# locations only\n",
    "DF_loc = DF_combined[DF_combined['location_yes'] > 0]\n",
    "DF_loc = DF_loc[['person1', 'person2']].reset_index(drop = True)\n",
    "\n",
    "cooc_loc = DF_loc.groupby(by=[\"person1\", \"person2\"], as_index=False).size()\n",
    "\n",
    "# family only\n",
    "DF_fam = DF_combined[DF_combined['family_yes'] > 0]\n",
    "DF_fam = DF_fam[['person1', 'person2']].reset_index(drop = True)\n",
    "\n",
    "cooc_fam = DF_fam.groupby(by=[\"person1\", \"person2\"], as_index=False).size()\n",
    "\n",
    "# take out issues where entity 1 == entity 2\n",
    "cooc_loc = cooc_loc[cooc_loc['person1'] != cooc_loc['person2']]\n",
    "cooc_fam = cooc_fam[cooc_fam['person1'] != cooc_fam['person2']]\n",
    "\n",
    "print(cooc_loc.head())\n",
    "print(cooc_fam.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cooc_loc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# start by plotting the whole thing for location \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cooc_loc_small \u001b[38;5;241m=\u001b[39m \u001b[43mcooc_loc\u001b[49m[cooc_loc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      3\u001b[0m graph \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mfrom_pandas_edgelist(\n\u001b[0;32m      4\u001b[0m            cooc_loc_small[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperson1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperson2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m]] \\\n\u001b[0;32m      5\u001b[0m            \u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m}),\n\u001b[0;32m      6\u001b[0m            source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperson1\u001b[39m\u001b[38;5;124m'\u001b[39m, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperson2\u001b[39m\u001b[38;5;124m'\u001b[39m, edge_attr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m pos \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mkamada_kawai_layout(graph, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cooc_loc' is not defined"
     ]
    }
   ],
   "source": [
    "# start by plotting the whole thing for location \n",
    "cooc_loc_small = cooc_loc[cooc_loc['size']>1]\n",
    "graph = nx.from_pandas_edgelist(\n",
    "           cooc_loc_small[['person1', 'person2', 'size']] \\\n",
    "           .rename(columns={'size': 'weight'}),\n",
    "           source='person1', target='person2', edge_attr=True)\n",
    "\n",
    "pos = nx.kamada_kawai_layout(graph, weight='weight')\n",
    "\n",
    "_ = plt.figure(figsize=(20, 20))\n",
    "nx.draw(graph, pos, \n",
    "        node_size=1000, \n",
    "        node_color='skyblue',\n",
    "        alpha=0.8,\n",
    "        with_labels = True)\n",
    "plt.title('Graph Visualization', size=15)\n",
    "\n",
    "for (node1,node2,data) in graph.edges(data=True):\n",
    "    width = data['weight'] \n",
    "    _ = nx.draw_networkx_edges(graph,pos,\n",
    "                               edgelist=[(node1, node2)],\n",
    "                               width=width,\n",
    "                               edge_color='#505050',\n",
    "                               alpha=0.5)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "# start by plotting the whole thing for location \n",
    "graph = nx.from_pandas_edgelist(\n",
    "           cooc_fam[['person1', 'person2', 'size']] \\\n",
    "           .rename(columns={'size': 'weight'}),\n",
    "           source='person1', target='person2', edge_attr=True)\n",
    "\n",
    "pos = nx.kamada_kawai_layout(graph, weight='weight')\n",
    "\n",
    "_ = plt.figure(figsize=(20, 20))\n",
    "nx.draw(graph, pos, \n",
    "        node_size=1000, \n",
    "        node_color='skyblue',\n",
    "        alpha=0.8,\n",
    "        with_labels = True)\n",
    "plt.title('Graph Visualization', size=15)\n",
    "\n",
    "for (node1,node2,data) in graph.edges(data=True):\n",
    "    width = data['weight'] \n",
    "    _ = nx.draw_networkx_edges(graph,pos,\n",
    "                               edgelist=[(node1, node2)],\n",
    "                               width=width,\n",
    "                               edge_color='#505050',\n",
    "                               alpha=0.5)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pysrt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpysrt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pysrt'"
     ]
    }
   ],
   "source": [
    "import pysrt\n",
    "import pandas as pd\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# install faiss-cpu\n",
    "import faiss\n",
    "import time\n",
    "subs = pysrt.open(\"bodies.srt\")\n",
    "\n",
    "DF = pd.DataFrame([\n",
    "  {\n",
    "    \"Text\": sub.text\n",
    "} for sub in subs])\n",
    "\n",
    "DF\n",
    "def remove_noise(text):\n",
    "    text = re.sub(\"<.*>\", \" \", text)\n",
    "    text = re.sub(\"{.*}\", \" \", text)\n",
    "    text = re.sub(\"\\[.*\\]\", \" \", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "DF['clean'] = DF['Text'].apply(remove_noise)\n",
    "\n",
    "DF = DF[DF['clean'] != \"\"]\n",
    "\n",
    "DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# this is creating the embeddings \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmsmarco-MiniLM-L-12-v3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m bodies_text_embds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(DF[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list())\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create an index using FAISS\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SentenceTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "# this is creating the embeddings \n",
    "model = SentenceTransformer('msmarco-MiniLM-L-12-v3')\n",
    "bodies_text_embds = model.encode(DF['clean'].to_list())\n",
    "# Create an index using FAISS\n",
    "index = faiss.IndexFlatL2(bodies_text_embds.shape[1])\n",
    "index.add(bodies_text_embds)\n",
    "faiss.write_index(index, 'index_bodies')\n",
    "\n",
    "bodies_text_embds\n",
    "# define a search \n",
    "def search(query, k):\n",
    "    \n",
    "    t=time.time()\n",
    "    query_vector = model.encode([query])\n",
    "    top_k = index.search(query_vector, k)\n",
    "    print('totaltime: {}'.format(time.time()-t))\n",
    "    return [DF['clean'].to_list()[_id] for _id in top_k[1].tolist()[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"cop\", 10)\n",
    "search(\"gun\", 10)\n",
    "search(\"car\", 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

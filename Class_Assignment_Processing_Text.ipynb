{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b89d485b",
   "metadata": {},
   "source": [
    "# Installation of Software\n",
    "-\tWhich should you use?\n",
    "    \n",
    "    -\tR/RStudio: mostly for users who are good with R and want to keep using RStudio because you've been using it a lot for other classes\n",
    "    -\tstart a rmarkdown assignment and make the assignment\n",
    "    o\tAnaconda/Jupyter/Spyder: you are already use python in this system a lot\n",
    "    -\tStart a jupyter notebook and make the assignment\n",
    "    o\tDatalore: is for people who are newer at python OR bad at installing stuff on their computer OR have an older computer OR have spaces in your username OR you want to be able to work together with your team mates or me\n",
    "    -\tstart a new notebook and make the assignment\n",
    "-\tNotes:\n",
    "    \n",
    "    o\tYou only need keras and tensorflow if you want to try deep learning in the classification section\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e4d288",
   "metadata": {},
   "source": [
    "# Pycharm\n",
    "\n",
    "- \tInstall!\n",
    "-\tClick learn\n",
    "-\tNew course\n",
    "-\tIn marketplace > introduction to python\n",
    "-\tTake a screen when you are done with the sections required (not all due at once)\n",
    "-\tBe sure you can see the TERMINAL at the bottom in that screen shot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a9a9d",
   "metadata": {},
   "source": [
    "# Processing Text\n",
    "\n",
    "- On class assignments, we all turn in the the same code and data source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d1609e",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f7139b-e29a-498d-b0f4-f529ba039261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user7\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import just a function \n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import a whole library as a new name\n",
    "import pandas as pd\n",
    "\n",
    "# import package as its own name\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# other packages\n",
    "import re\n",
    "\n",
    "\n",
    "# rest of stuff\n",
    "import textacy.preprocessing as tprep\n",
    "\n",
    "# install pyspellchecker !!!\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import spacy\n",
    "\n",
    "import textacy\n",
    "\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# impurity function\n",
    "RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')\n",
    "\n",
    "def impurity(text, min_len=10):\n",
    "    \"\"\"returns the share of suspicious characters in a text\"\"\"\n",
    "    if text == None or len(text) < min_len:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(RE_SUSPICIOUS.findall(text))/len(text)\n",
    "\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    text = tprep.replace.phone_numbers(text)\n",
    "    text = tprep.replace.urls(text)\n",
    "    text = tprep.replace.emails(text)\n",
    "    text = tprep.replace.user_handles(text)\n",
    "    text = tprep.replace.emojis(text)\n",
    "    return text\n",
    "    \n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffe3860",
   "metadata": {},
   "source": [
    "## Find Text\n",
    "-\tAs a class, we will find a text source to analyze. This text source usually will consist of a webpage or other dataset to examine and clean.\n",
    "-\tImport the text into your report.\n",
    "-\tIf the text is one big long string, first break into sentence segments and store it in a Pandas DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd91eff-70bc-4e83-821e-674831386341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nPatrick Mahomes' fiery message after win over Bills: 'They got what they asked for' | Fox News\\n\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myurl = \"https://www.foxnews.com/sports/patrick-mahomes-fiery-message-win-bills-they-got-what-they-asked-for\"\n",
    "#myurl = \"https://www.foxnews.com/lifestyle/newly-elected-school-board-pennsylvania-reclaims-indigenous-mascot-rejects-cancel-culture\"\n",
    "\n",
    "html = urlopen(myurl).read()\n",
    "\n",
    "soupified = BeautifulSoup(html, \"html.parser\")\n",
    "# soupified\n",
    "\n",
    "# just try get_text()\n",
    "try_text = soupified.get_text()\n",
    "try_text[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a268d11",
   "metadata": {},
   "source": [
    "-  Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccac10e9-d903-42b3-a527-601b3b997ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find an exact match for the first time this occurs\n",
    "text = try_text[\n",
    "  # everything from the end of this sentence and on\n",
    "  re.search(\"To access the content, check your email and follow the instructions provided.\", try_text).end():\n",
    "  # now the end\n",
    "  re.search(\"CLICK HERE TO GET THE FOX NEWS APP\", try_text).start()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a6908",
   "metadata": {},
   "source": [
    "- •\tBreaking down into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af4769e8-2d61-4515-bb86-c2757800aa11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n       Having trouble?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Click here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kansas City Chiefs quarterback Patrick Mahomes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mahomes, surrounded by Travis Kelce, Chris Jon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They have the Baltimore Ravens up next.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence\n",
       "0                           \\n       Having trouble?\n",
       "1                                        Click here.\n",
       "2  Kansas City Chiefs quarterback Patrick Mahomes...\n",
       "3  Mahomes, surrounded by Travis Kelce, Chris Jon...\n",
       "4            They have the Baltimore Ravens up next."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# break down into sentences and put into DF\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "type(sentences)\n",
    "\n",
    "# convert to dataframe\n",
    "DF = pd.DataFrame(sentences, columns = [\"sentence\"])\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec57a9c",
   "metadata": {},
   "source": [
    "-\tWe've used:\n",
    "\n",
    "    o\tOne big string (one variable)\n",
    "\n",
    "    o\tA list which uses []\n",
    "\n",
    "    o\tDictionaries {}\n",
    "\n",
    "    o\tTuples ()\n",
    "    \n",
    "    o\tDataFrame from pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d9164",
   "metadata": {},
   "source": [
    "## Length for Proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0ff3ccf-8b1b-4540-b2a0-3241342234f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "554"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do this on the full text not broken into sentences\n",
    "len(nltk.word_tokenize(text))\n",
    "# be sure to import nltk in the proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef9878",
   "metadata": {},
   "source": [
    "## Fix Errors\n",
    "-\tExamine the text for errors or problems by looking at the text.\n",
    "    \n",
    "    o\tLegit, just look at the text.\n",
    "    \n",
    "    o\tLooking for any type of \"garbage\" - dependent on what you are doing.\n",
    "-\tUse the “impurity” function from class to examine the text for potential issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7230ae7d-073c-4062-bc08-bf88b74e30bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n       Having trouble?</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Click here.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kansas City Chiefs quarterback Patrick Mahomes...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mahomes, surrounded by Travis Kelce, Chris Jon...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They have the Baltimore Ravens up next.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Hey, they asked for it, and they got what the...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"But he (Andy Reid) said it.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This s--- ain’t done.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We come back next week ready to f---ing go.\"</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CLICK HERE FOR MORE SPORTS COVERAGE ON FOXNEWS...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>VIEW THE MOMENT ON X.Reid said it’s not time f...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>They still have to get through the Ravens and ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Ravens are going to be the Chiefs’ toughes...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Kansas City will have to do it on the road.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Ravens are hosting a conference championsh...</td>\n",
       "      <td>0.002222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Baltimore boasts one of the toughest defenses ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"There’s no weakness there,\" the star quarterb...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\"It’s going to take our best effort.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Defense, offense, special teams.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>They do it all.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>It’s always a great challenge, and that stadiu...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>So, we’re excited for the challenge.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"On Monday, Mahomes added that he praised the ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"For three quarters offensively, we were movin...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\"I went over to the defense and told them: ‘Y’...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>We’ll get to the AFC championship game.\"</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Kansas City Chiefs quarterback Patrick Mahomes...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence     score\n",
       "0                            \\n       Having trouble?  0.000000\n",
       "1                                         Click here.  0.000000\n",
       "2   Kansas City Chiefs quarterback Patrick Mahomes...  0.000000\n",
       "3   Mahomes, surrounded by Travis Kelce, Chris Jon...  0.000000\n",
       "4             They have the Baltimore Ravens up next.  0.000000\n",
       "5   \"Hey, they asked for it, and they got what the...  0.000000\n",
       "6                        \"But he (Andy Reid) said it.  0.000000\n",
       "7                               This s--- ain’t done.  0.000000\n",
       "8        We come back next week ready to f---ing go.\"  0.000000\n",
       "9   CLICK HERE FOR MORE SPORTS COVERAGE ON FOXNEWS...  0.000000\n",
       "10  VIEW THE MOMENT ON X.Reid said it’s not time f...  0.000000\n",
       "11  They still have to get through the Ravens and ...  0.000000\n",
       "12  The Ravens are going to be the Chiefs’ toughes...  0.000000\n",
       "13        Kansas City will have to do it on the road.  0.000000\n",
       "14  The Ravens are hosting a conference championsh...  0.002222\n",
       "15  Baltimore boasts one of the toughest defenses ...  0.000000\n",
       "16  \"There’s no weakness there,\" the star quarterb...  0.000000\n",
       "17               \"It’s going to take our best effort.  0.000000\n",
       "18                   Defense, offense, special teams.  0.000000\n",
       "19                                    They do it all.  0.000000\n",
       "20  It’s always a great challenge, and that stadiu...  0.000000\n",
       "21               So, we’re excited for the challenge.  0.000000\n",
       "22  \"On Monday, Mahomes added that he praised the ...  0.000000\n",
       "23  \"For three quarters offensively, we were movin...  0.000000\n",
       "24  \"I went over to the defense and told them: ‘Y’...  0.000000\n",
       "25           We’ll get to the AFC championship game.\"  0.000000\n",
       "26  Kansas City Chiefs quarterback Patrick Mahomes...  0.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF['score'] = DF['sentence'].apply(impurity)\n",
    "DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8fa194",
   "metadata": {},
   "source": [
    "-\tRemove the noise with the regex function.\n",
    "\n",
    "-\tRe-examine the impurity to determine if the data has been mostly cleaned.\n",
    "\n",
    "    o\tNot necessary because it looks fine.\n",
    "\n",
    "-\tNormalize the rest of the text by using textacy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc289efb-dd5b-4bb8-bc9d-e66c365f1ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n       Having trouble?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\\n       Having trouble?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Click here.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Click here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kansas City Chiefs quarterback Patrick Mahomes...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Kansas City Chiefs quarterback Patrick Mahomes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mahomes, surrounded by Travis Kelce, Chris Jon...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Mahomes, surrounded by Travis Kelce, Chris Jon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They have the Baltimore Ravens up next.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>They have the Baltimore Ravens up next.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Hey, they asked for it, and they got what the...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"Hey, they asked for it, and they got what the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"But he (Andy Reid) said it.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"But he (Andy Reid) said it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This s--- ain’t done.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This s--- ain't done.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We come back next week ready to f---ing go.\"</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>We come back next week ready to f---ing go.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CLICK HERE FOR MORE SPORTS COVERAGE ON FOXNEWS...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>CLICK HERE FOR MORE SPORTS COVERAGE ON FOXNEWS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>VIEW THE MOMENT ON X.Reid said it’s not time f...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>VIEW THE MOMENT ON X.Reid said it's not time f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>They still have to get through the Ravens and ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>They still have to get through the Ravens and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Ravens are going to be the Chiefs’ toughes...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>The Ravens are going to be the Chiefs' toughes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Kansas City will have to do it on the road.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Kansas City will have to do it on the road.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Ravens are hosting a conference championsh...</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>The Ravens are hosting a conference championsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Baltimore boasts one of the toughest defenses ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Baltimore boasts one of the toughest defenses ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"There’s no weakness there,\" the star quarterb...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"There's no weakness there,\" the star quarterb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\"It’s going to take our best effort.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"It's going to take our best effort.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Defense, offense, special teams.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Defense, offense, special teams.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>They do it all.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>They do it all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>It’s always a great challenge, and that stadiu...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>It's always a great challenge, and that stadiu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>So, we’re excited for the challenge.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>So, we're excited for the challenge.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"On Monday, Mahomes added that he praised the ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"On Monday, Mahomes added that he praised the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"For three quarters offensively, we were movin...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"For three quarters offensively, we were movin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\"I went over to the defense and told them: ‘Y’...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>\"I went over to the defense and told them: 'Y'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>We’ll get to the AFC championship game.\"</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>We'll get to the AFC championship game.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Kansas City Chiefs quarterback Patrick Mahomes...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Kansas City Chiefs quarterback Patrick Mahomes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence     score  \\\n",
       "0                            \\n       Having trouble?  0.000000   \n",
       "1                                         Click here.  0.000000   \n",
       "2   Kansas City Chiefs quarterback Patrick Mahomes...  0.000000   \n",
       "3   Mahomes, surrounded by Travis Kelce, Chris Jon...  0.000000   \n",
       "4             They have the Baltimore Ravens up next.  0.000000   \n",
       "5   \"Hey, they asked for it, and they got what the...  0.000000   \n",
       "6                        \"But he (Andy Reid) said it.  0.000000   \n",
       "7                               This s--- ain’t done.  0.000000   \n",
       "8        We come back next week ready to f---ing go.\"  0.000000   \n",
       "9   CLICK HERE FOR MORE SPORTS COVERAGE ON FOXNEWS...  0.000000   \n",
       "10  VIEW THE MOMENT ON X.Reid said it’s not time f...  0.000000   \n",
       "11  They still have to get through the Ravens and ...  0.000000   \n",
       "12  The Ravens are going to be the Chiefs’ toughes...  0.000000   \n",
       "13        Kansas City will have to do it on the road.  0.000000   \n",
       "14  The Ravens are hosting a conference championsh...  0.002222   \n",
       "15  Baltimore boasts one of the toughest defenses ...  0.000000   \n",
       "16  \"There’s no weakness there,\" the star quarterb...  0.000000   \n",
       "17               \"It’s going to take our best effort.  0.000000   \n",
       "18                   Defense, offense, special teams.  0.000000   \n",
       "19                                    They do it all.  0.000000   \n",
       "20  It’s always a great challenge, and that stadiu...  0.000000   \n",
       "21               So, we’re excited for the challenge.  0.000000   \n",
       "22  \"On Monday, Mahomes added that he praised the ...  0.000000   \n",
       "23  \"For three quarters offensively, we were movin...  0.000000   \n",
       "24  \"I went over to the defense and told them: ‘Y’...  0.000000   \n",
       "25           We’ll get to the AFC championship game.\"  0.000000   \n",
       "26  Kansas City Chiefs quarterback Patrick Mahomes...  0.000000   \n",
       "\n",
       "                                                clean  \n",
       "0                            \\n       Having trouble?  \n",
       "1                                         Click here.  \n",
       "2   Kansas City Chiefs quarterback Patrick Mahomes...  \n",
       "3   Mahomes, surrounded by Travis Kelce, Chris Jon...  \n",
       "4             They have the Baltimore Ravens up next.  \n",
       "5   \"Hey, they asked for it, and they got what the...  \n",
       "6                        \"But he (Andy Reid) said it.  \n",
       "7                               This s--- ain't done.  \n",
       "8        We come back next week ready to f---ing go.\"  \n",
       "9   CLICK HERE FOR MORE SPORTS COVERAGE ON FOXNEWS...  \n",
       "10  VIEW THE MOMENT ON X.Reid said it's not time f...  \n",
       "11  They still have to get through the Ravens and ...  \n",
       "12  The Ravens are going to be the Chiefs' toughes...  \n",
       "13        Kansas City will have to do it on the road.  \n",
       "14  The Ravens are hosting a conference championsh...  \n",
       "15  Baltimore boasts one of the toughest defenses ...  \n",
       "16  \"There's no weakness there,\" the star quarterb...  \n",
       "17               \"It's going to take our best effort.  \n",
       "18                   Defense, offense, special teams.  \n",
       "19                                    They do it all.  \n",
       "20  It's always a great challenge, and that stadiu...  \n",
       "21               So, we're excited for the challenge.  \n",
       "22  \"On Monday, Mahomes added that he praised the ...  \n",
       "23  \"For three quarters offensively, we were movin...  \n",
       "24  \"I went over to the defense and told them: 'Y'...  \n",
       "25           We'll get to the AFC championship game.\"  \n",
       "26  Kansas City Chiefs quarterback Patrick Mahomes...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF['clean'] = DF['sentence'].apply(normalize)\n",
    "DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedceb2e",
   "metadata": {},
   "source": [
    "- \tExamine spelling errors in at least one row of the dataset.\n",
    "\n",
    "    o\tAny time you have stuff with names, please do not do spelling.\n",
    "\n",
    "    o\tMostly, only do this if you have a specific goals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f14afea9-a540-4f4f-929a-69d2d216f788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ii\n",
      "\n",
      "\n",
      "i\n",
      "{'in', 'if', 'is', 'id', 'i', 'ai', 'bi', 'hi', 'it', 'ti', 'qi', 'si', 'ki', 'oi', 'pi', 'li', 'mi', 'xi'}\n",
      "baltimore\n",
      "\n",
      "\n",
      "None\n",
      "None\n",
      "photo/frank\n",
      "\n",
      "\n",
      "None\n",
      "None\n",
      "photo/adrian\n",
      "\n",
      "\n",
      "None\n",
      "None\n",
      "lamar\n",
      "\n",
      "\n",
      "lama\n",
      "{'lazar', 'lama', 'damar', 'lamer', 'lamas'}\n",
      "jim\n",
      "\n",
      "\n",
      "him\n",
      "{'jib', 'him', 'jam', 'rim', 'jig', 'nim', 'vim', 'mim', 'aim', 'dim'}\n",
      "'s\n",
      "\n",
      "\n",
      "is\n",
      "{'is', 'vs', 'cs', 'ts', 'ls', 'ps', 's', 'ss', 'gs', 'ms', 'es', 'rs', 'ks', 'as', 'us'}\n",
      "'re\n",
      "\n",
      "\n",
      "are\n",
      "{'ore', 'ire', 'ere', 're', 'are'}\n",
      "travis\n",
      "\n",
      "\n",
      "traves\n",
      "{'traves'}\n",
      "monday\n",
      "\n",
      "\n",
      "money\n",
      "{'money', 'moray', 'monads', 'moldy', 'moody', 'mayday', 'monody', 'monas', 'modal', 'today', 'monde', 'monad', 'midday', 'fondly', 'monkey', 'noonday'}\n",
      "chris\n",
      "\n",
      "\n",
      "chis\n",
      "{'chrism', 'chis'}\n",
      "kelce\n",
      "\n",
      "\n",
      "else\n",
      "{'kelt', 'ketch', 'fence', 'deuce', 'kelter', 'belle', 'kale', 'else', 'veloce', 'kerne', 'kedge', 'kelpie', 'kyle', 'helve', 'belch', 'kelly', 'pence', 'belie', 'dolce', 'hence', 'welch', 'recce', 'deice', 'keel', 'peace', 'delve', 'keck', 'kente', 'kelp', 'terce', 'dele'}\n",
      "jan.\n",
      "\n",
      "\n",
      "jane\n",
      "{'jane'}\n",
      "harbaugh\n",
      "\n",
      "\n",
      "None\n",
      "None\n",
      "nfl\n",
      "\n",
      "\n",
      "nil\n",
      "{'nil'}\n",
      "brittany\n",
      "\n",
      "\n",
      "dittany\n",
      "{'dittany'}\n",
      "jones\n",
      "\n",
      "\n",
      "ones\n",
      "{'zones', 'cones', 'jokes', 'nones', 'ones', 'bones', 'tones', 'pones', 'hones', 'sones'}\n",
      "``\n",
      "\n",
      "\n",
      "i\n",
      "{'is', 'hg', 'mb', 'it', 'h', 'd', 'am', 'ut', 'uh', 'kb', 'ln', 'k', 'la', 'em', 'ka', 'j', 'jo', 'if', 'ha', 'ye', 'vs', 'ls', 'or', 'w', 'xu', 'at', 'id', 'dm', 'v', 'we', 'mi', 'go', 'p', 'q', 't', 'ps', 'de', 'fa', 'a', 'ow', 're', 'as', 'i', 's', 'jg', 'ne', 'ks', 'lx', 'um', 'er', 'ai', 'mu', 'cs', 'ah', 'od', 'ti', 'm', 'en', 'lm', 'me', 'be', 'ja', 'el', 'of', 'g', 'wo', 'b', 'dg', 'on', 'us', 'dc', 'to', 'kw', 'bi', 'hi', 'no', 'mf', 'so', 'hm', 'my', 'n', 'yo', 'o', 'ax', 'db', 'ox', 'ad', 'rs', 'eh', 'ta', 'sh', 'pa', 'si', 'y', 'oi', 'pi', 'ms', 'by', 'cw', 'r', 'do', 'ho', 'qi', 'nu', 'es', 'hl', 'ss', 'l', 'x', 'oh', 'ts', 'ex', 'u', 'c', 'kc', 'ya', 'an', 'om', 'lo', 'ki', 'li', 'et', 'z', 'in', 'pe', 'f', 'gs', 'ma', 'up', 'e', 'oo', 'he', 'ay', 'xi'}\n",
      "x.reid\n",
      "\n",
      "\n",
      "nereid\n",
      "{'nereid'}\n",
      "-ing\n",
      "\n",
      "\n",
      "sing\n",
      "{'sing', 'ting', 'wing', 'ding', 'zing', 'ping', 'ring', 'king'}\n",
      "n't\n",
      "\n",
      "\n",
      "not\n",
      "{'net', \"an't\", 'not', 'nut', 'nit'}\n",
      "kansas\n",
      "\n",
      "\n",
      "canvas\n",
      "{'tankas', 'pandas', 'cannas', 'kinas', 'canvas', 'manias', 'paisas', 'kwanzas', 'manses', 'anoas', 'vandas', 'kashas', 'ganjas', 'mantas', 'manas', 'kana', 'kantar', 'kavas', 'anas', 'kappas', 'balsas', 'pangas', 'kanzus', 'annas', 'salsas'}\n",
      "sunday\n",
      "\n",
      "\n",
      "sundae\n",
      "{'sundae', 'sundry'}\n",
      "reid\n",
      "\n",
      "\n",
      "read\n",
      "{'reis', 'raid', 'redid', 'rend', 'rid', 'reed', 'read', 'rebid', 'redd', 'rein', 'red'}\n",
      "ap\n",
      "\n",
      "\n",
      "a\n",
      "{'tap', 'am', 'nap', 'a', 'as', 'amp', 'cap', 'rap', 'sap', 'alp', 'ai', 'lap', 'ah', 'ape', 'ax', 'hap', 'an', 'ad', 'map', 'asp', 'zap', 'at', 'apt', 'bap', 'pa', 'up', 'yap', 'gap', 'pap', 'dap', 'ay', 'p'}\n",
      "'ll\n",
      "\n",
      "\n",
      "all\n",
      "{'ell', 'all', 'ill'}\n",
      "andy\n",
      "\n",
      "\n",
      "and\n",
      "{'handy', 'and', 'any', 'dandy', 'randy', 'candy', 'bandy', 'sandy'}\n",
      "--\n",
      "\n",
      "\n",
      "i\n",
      "{'is', 'hg', 'mb', 'it', 'h', 'd', 'am', 'ut', 'uh', 'kb', 'ln', 'k', 'la', 'em', 'j', 'ka', 'jo', 'if', 'ha', 'ye', 'vs', 'ls', 'or', 'w', 'xu', 'at', 'dm', 'id', 'v', 'we', 'mi', 'go', 'p', 'q', 't', 'de', 'ps', 'fa', 'a', 'ow', 're', 'as', 'i', 's', 'jg', 'ne', 'ks', 'lx', 'um', 'er', 'ai', 'mu', 'cs', 'ah', 'od', 'ti', 'm', 'en', 'lm', 'me', 'be', 'ja', 'el', 'of', 'g', 'wo', 'b', 'dg', 'on', 'us', 'dc', 'to', 'kw', 'bi', 'hi', 'mf', 'no', 'so', 'hm', 'my', 'n', 'yo', 'o', 'ax', 'db', 'ox', 'ad', 'rs', 'eh', 'ta', 'sh', 'pa', 'y', 'si', 'oi', 'ms', 'pi', 'by', 'cw', 'r', 'do', 'ho', 'qi', 'nu', 'es', 'hl', 'ss', 'l', 'x', 'oh', 'ts', 'ex', 'u', 'c', 'om', 'kc', 'ya', 'an', 'ki', 'lo', 'li', 'et', 'z', 'in', 'pe', 'f', 'gs', 'ma', 'up', 'e', 'oo', 'he', 'ay', 'xi'}\n",
      "night.chargers\n",
      "\n",
      "\n",
      "None\n",
      "None\n",
      "n.y.\n",
      "\n",
      "\n",
      "nay\n",
      "{'nay', 'nays'}\n",
      "afc\n",
      "\n",
      "\n",
      "aft\n",
      "{'aft', 'arc'}\n",
      "''\n",
      "\n",
      "\n",
      "i\n",
      "{'is', 'hg', 'mb', 'it', 'h', 'd', 'am', 'ut', 'uh', 'kb', 'ln', 'k', 'la', 'em', 'j', 'ka', 'jo', 'if', 'ha', 'ye', 'vs', 'ls', 'or', 'w', 'xu', 'at', 'dm', 'id', 'v', 'we', 'mi', 'go', 'p', 'q', 't', 'de', 'ps', 'fa', 'a', 'ow', 're', 'as', 'i', 's', 'ne', 'jg', 'ks', 'lx', 'er', 'um', 'ai', 'cs', 'mu', 'ah', 'od', 'ti', 'm', 'en', 'lm', 'me', 'be', 'ja', 'el', 'of', 'g', \"h'm\", 'wo', 'b', 'dg', 'on', 'us', 'dc', 'to', 'kw', 'bi', 'hi', 'no', 'mf', 'so', 'hm', 'my', 'n', 'yo', 'o', 'ax', 'db', 'ox', 'ad', 'rs', 'eh', 'ta', 'sh', 'pa', 'y', 'si', 'oi', 'ms', 'pi', 'by', 'cw', 'r', 'do', 'ho', 'nu', 'qi', 'es', 'hl', 'ss', 'l', 'x', 'oh', 'ts', 'ex', 'u', 'c', 'kc', 'om', 'an', 'ya', 'ki', 'lo', 'li', 'et', 'z', 'in', 'pe', 'f', 'gs', 'ma', 'up', 'e', 'oo', 'he', 'ay', 'xi'}\n",
      "mvp\n",
      "\n",
      "\n",
      "map\n",
      "{'mop', 'map'}\n",
      "kraus\n",
      "\n",
      "\n",
      "krauts\n",
      "{'krauts', 'kraut'}\n",
      "jackson\n",
      "\n",
      "\n",
      "jacks\n",
      "{'jacks', 'jackpot'}\n",
      "mahomes\n",
      "\n",
      "\n",
      "homes\n",
      "{'homes', 'manholes', 'mahouts', 'radomes'}\n",
      "patrick\n",
      "\n",
      "\n",
      "trick\n",
      "{'pathic', 'iatric', 'patriot', 'strick', 'trick', 'hayrick', 'prick'}\n",
      "foxnews.com\n",
      "\n",
      "\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# find all the unique tokens\n",
    "# set is find unique\n",
    "# nltk.word_tokenize is break down into words\n",
    "# \" \".join is combine into one long text\n",
    "# .to_list() is a function to convert to list \n",
    "clean_tokens = set(nltk.word_tokenize(\" \".join(DF['clean'].to_list())))\n",
    "\n",
    "# what is wrong? \n",
    "misspelled = spell.unknown(clean_tokens)\n",
    "\n",
    "for word in misspelled:\n",
    "    #  what's the word\n",
    "    print(word)\n",
    "    print(\"\\n\")\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # Get a list of `likely` options\n",
    "    print(spell.candidates(word))\n",
    "    \n",
    "# make a dictionary of the misspelled word and the correction\n",
    "# use find and replace in re to fix them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e3f1d",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "-\tUsing spacy and textacy, pre-process the text to end up with a list of tokenized lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "644e825d-1aa1-4f82-8362-1a51cb307d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Having', 'trouble']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = []\n",
    "\n",
    "# only the tagger and lemmatizer\n",
    "for doc in nlp.pipe(DF['clean'].tolist(), disable=[\"tok2vec\", \"ner\", \"parser\"]):\n",
    "  tokens = textacy.extract.words(doc,\n",
    "            filter_stops = True,           # default True, no stopwords\n",
    "            filter_punct = True,           # default True, no punctuation\n",
    "            filter_nums = True,            # default False, no numbers\n",
    "            include_pos = None,            # default None = include all\n",
    "            exclude_pos = None,            # default None = exclude none\n",
    "            min_freq = 1)                  # minimum frequency of words\n",
    "  output.append([str(word) for word in tokens]) # close output append \n",
    "\n",
    "output[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc95e3b",
   "metadata": {},
   "source": [
    "•\tCreate a frequency table of each of the tokens returned in this output. Below is some example code to get us started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "006e1cbd-3b02-40f6-b986-9e57584eeb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Mahomes': 9,\n",
       "         'Chiefs': 6,\n",
       "         'game': 6,\n",
       "         'Kansas': 5,\n",
       "         'City': 5,\n",
       "         'quarterback': 5,\n",
       "         'Bills': 5,\n",
       "         'AFC': 5,\n",
       "         'Sunday': 5,\n",
       "         'said': 5,\n",
       "         'Patrick': 4,\n",
       "         'Buffalo': 4,\n",
       "         'Ravens': 4,\n",
       "         'championship': 3,\n",
       "         'got': 3,\n",
       "         'divisional': 3,\n",
       "         'playoff': 3,\n",
       "         'Jan.': 3,\n",
       "         'Orchard': 3,\n",
       "         'Park': 3,\n",
       "         'N.Y.': 3,\n",
       "         'AP': 3,\n",
       "         'Photo': 3,\n",
       "         'going': 3,\n",
       "         'Kelce': 2,\n",
       "         'Andy': 2,\n",
       "         'Reid': 2,\n",
       "         'Baltimore': 2,\n",
       "         'asked': 2,\n",
       "         'week': 2,\n",
       "         'Adrian': 2,\n",
       "         'Kraus': 2,\n",
       "         'added': 2,\n",
       "         'time': 2,\n",
       "         'win': 2,\n",
       "         'toughest': 2,\n",
       "         'NFL': 2,\n",
       "         'offense': 2,\n",
       "         'challenge': 2,\n",
       "         'defense': 2,\n",
       "         'shut': 2,\n",
       "         'quarter': 2,\n",
       "         'Having': 1,\n",
       "         'trouble': 1,\n",
       "         'Click': 1,\n",
       "         'delivered': 1,\n",
       "         'epic': 1,\n",
       "         'pep': 1,\n",
       "         'talk': 1,\n",
       "         'teammates': 1,\n",
       "         'team': 1,\n",
       "         'defeated': 1,\n",
       "         'advance': 1,\n",
       "         'surrounded': 1,\n",
       "         'Travis': 1,\n",
       "         'Chris': 1,\n",
       "         'Jones': 1,\n",
       "         'agreed': 1,\n",
       "         'coach': 1,\n",
       "         'job': 1,\n",
       "         'Hey': 1,\n",
       "         's---': 1,\n",
       "         'ai': 1,\n",
       "         'come': 1,\n",
       "         'ready': 1,\n",
       "         'f': 1,\n",
       "         'ing': 1,\n",
       "         'CLICK': 1,\n",
       "         'SPORTS': 1,\n",
       "         'COVERAGE': 1,\n",
       "         'FOXNEWS.COM': 1,\n",
       "         'reacts': 1,\n",
       "         'beating': 1,\n",
       "         'case': 1,\n",
       "         'know': 1,\n",
       "         'VIEW': 1,\n",
       "         'MOMENT': 1,\n",
       "         'X.Reid': 1,\n",
       "         'dancing': 1,\n",
       "         'Super': 1,\n",
       "         'Bowl': 1,\n",
       "         'celebrations': 1,\n",
       "         'happen': 1,\n",
       "         'matchup': 1,\n",
       "         'road': 1,\n",
       "         'hosting': 1,\n",
       "         'conference': 1,\n",
       "         'history': 1,\n",
       "         'M&T': 1,\n",
       "         'Bank': 1,\n",
       "         'Stadium': 1,\n",
       "         'night': 1,\n",
       "         'CHARGERS': 1,\n",
       "         'PURSUIT': 1,\n",
       "         'JIM': 1,\n",
       "         'HARBAUGH': 1,\n",
       "         'ADVANCING': 1,\n",
       "         'FINAL': 1,\n",
       "         'STAGES': 1,\n",
       "         'REPORT': 1,\n",
       "         'Brittany': 1,\n",
       "         'right': 1,\n",
       "         'hugs': 1,\n",
       "         'knows': 1,\n",
       "         'work': 1,\n",
       "         'cut': 1,\n",
       "         'boasts': 1,\n",
       "         'defenses': 1,\n",
       "         'likely': 1,\n",
       "         'MVP': 1,\n",
       "         'Lamar': 1,\n",
       "         'Jackson': 1,\n",
       "         'weakness': 1,\n",
       "         'star': 1,\n",
       "         'best': 1,\n",
       "         'effort': 1,\n",
       "         'Defense': 1,\n",
       "         'special': 1,\n",
       "         'teams': 1,\n",
       "         'great': 1,\n",
       "         'stadium': 1,\n",
       "         'rocking': 1,\n",
       "         'excited': 1,\n",
       "         'Monday': 1,\n",
       "         'praised': 1,\n",
       "         'coming': 1,\n",
       "         'clutch': 1,\n",
       "         'quarters': 1,\n",
       "         'offensively': 1,\n",
       "         'moving': 1,\n",
       "         'ball': 1,\n",
       "         'field': 1,\n",
       "         'went': 1,\n",
       "         'told': 1,\n",
       "         \"Y'all\": 1,\n",
       "         'football': 1,\n",
       "         'rolls': 1,\n",
       "         'pocket': 1,\n",
       "         'Frank': 1,\n",
       "         'Franklin': 1,\n",
       "         'II': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all items\n",
    "type(output)\n",
    "# first list\n",
    "type(output[0])\n",
    "# first list, first item (this is the issue!)\n",
    "type(output[0][0])\n",
    "\n",
    "Counter(chain.from_iterable(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ecd895",
   "metadata": {},
   "source": [
    "## Summarize\n",
    "\n",
    "    Write a paragraph explaining the process of cleaning data for an NLP pipeline. You should explain the errors you found in the dataset and how you fixed them. Explain the information that is gathered by using spacy and textacy and the final output. What did you learn from your frequency table? What is the text document about?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3faf3",
   "metadata": {},
   "source": [
    "To begin the data cleaning process for our NLP pipeline, we first encountered an error related to the NLTK library,\n",
    "where the punkt tokenizer models had not been downloaded, resulting in a LookupError. This issue was resolved by \n",
    "downloading the necessary punkt package using nltk.download('punkt'), which allowed us to tokenize our text data \n",
    "into sentences successfully.\n",
    "\n",
    "Digging deeper with tools like spacy and textacy, we got into the nitty-gritty of our text, uncovering the roles \n",
    "of words, spotting key terms and phrases, and mapping out how words connect to each other. This deeper dive into \n",
    "the language mechanics helped us spot and smooth out any rough patches or unclear bits in our text, giving us a \n",
    "clearer, more consistent dataset to work with.\n",
    "\n",
    "The final output of our cleaning process was a well-structured dataset, ready for more advanced NLP tasks. From \n",
    "the frequency table generated, we gained insights into the most common words and phrases, which helped us \n",
    "understand the underlying themes and topics of our text document. \n",
    "\n",
    "\n",
    "Overall, the process taught us the importance of thorough data cleaning and preparation in NLP. By addressing \n",
    "errors upfront and utilizing powerful linguistic tools, we were able to enhance the quality of our dataset, \n",
    "paving the way for more accurate and insightful natural language processing applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
